# fly.toml app configuration file generated for ollama-desculpa-ai on 2025-08-26T23:52:56-03:00
#
# See https://fly.io/docs/reference/configuration/ for information about how to use this file.
#

app = 'ollama-desculpa-ai'
primary_region = 'gru'

[build]
  dockerfile = './ollama/Dockerfile.app'

[http_service]
  internal_port = 11434
  force_https = true
  auto_stop_machines = 'stop'
  auto_start_machines = true
  min_machines_running = 0
  processes = ['app']

[[vm]]
  memory = '4gb'
  cpu_kind = 'shared'
  cpus = 4
